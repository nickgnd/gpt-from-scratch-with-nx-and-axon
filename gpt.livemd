# Let's build GPT: from scratch, in code, spelled out… but with Axon

```elixir
Mix.install(
  [
    {:nx, "~> 0.5.3"},
    {:axon, "~> 0.5"},
    {:exla, "~> 0.5.1"},
    {:kino, "~> 0.7.0"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Intro

Resources:

* https://www.youtube.com/watch?v=kCc8FmEb1nY
* https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=wJpXpmjEYC_T
* https://github.com/wtedw/gpt-from-scratch/blob/main/gpt-dev.livemd

## Prepare the data

```elixir
text =
  __DIR__
  |> Path.join("input.txt")
  |> Path.expand()
  |> File.read!()
```

### Our Vocabulary

```elixir
chars =
  text
  |> String.codepoints()
  |> Enum.uniq()
  |> Enum.sort()

vocab_size = length(chars)

IO.inspect(Enum.join(chars), label: "chars")
IO.inspect(vocab_size, label: "vocab_size")

:ok
```

## Basic Encoder / Decoder

We are going to develop some strategy to tokenize the input text: convert the raw text to some sequence of integers according to some vocabolary of possible elements.

In this case we are going to tokenize single chars: for each char there is a corresponding integer.

```elixir
defmodule Minidecoder do
  @text __DIR__
        |> Path.join("input.txt")
        |> Path.expand()
        |> File.read!()

  @chars String.codepoints(@text) |> Enum.uniq() |> Enum.sort()

  # string-to-integer
  @stoi Enum.reduce(@chars, %{}, fn ch, acc -> Map.put(acc, ch, Enum.count(acc)) end)

  # integer-to-string
  @itos Enum.reduce(@stoi, %{}, fn {ch, i}, acc -> Map.put(acc, i, ch) end)

  def vocab_size, do: length(@chars)

  def encode_char(char), do: @stoi[char]

  def decode_char(encoded_char), do: @itos[encoded_char]

  def encode(text) do
    text |> String.codepoints() |> Enum.map(&encode_char(&1))
  end

  def decode(encoded_list) do
    encoded_list |> Enum.map(&decode_char(&1)) |> Enum.join()
  end

  def tensor(text, opts \\ []) do
    Nx.tensor(encode(text), opts)
  end
end
```

```elixir
vocab_size =
  Minidecoder.vocab_size()
  |> IO.inspect(label: "vocab size is")

Minidecoder.encode("hii there") |> IO.inspect(label: "Encoding")
Minidecoder.encode("hii there") |> Minidecoder.decode() |> IO.inspect(label: "Decoding")

:ok
```

```elixir
data = Minidecoder.tensor(text, type: :s64)

Nx.shape(data) |> IO.inspect(label: "shape")
data[0..999] |> IO.inspect(label: "first 1000 chars")

:ok
```

### Separate dataset in train and validate dataset

```elixir
# Use the 90% of the data for training and the remaining for the validation
n = Kernel.round(Nx.size(data) * 0.9)

train_data = data[0..(n - 1)//1]
val_data = data[n..-1//1]

{train_data, val_data}
```

## Data loader: batches of chunk of data

To train the transform with split the train dataset in different chunks to speed it up.

```elixir
block_size = 8

# Please note that the range `[0..block_size]` is equivalent to `[:block_size+1]` in pyhton
chunk = train_data[0..block_size]

IO.inspect(chunk, label: "chunk")

# In a chunk of 9 chars, there are 8 individual example packed in there:
# 18 -> 47
# 18, 47 -> 56
# 18, 47, 56 -> 57
# …

x = train_data[0..(block_size - 1)]
y = train_data[1..block_size]

for t <- 0..(block_size - 1) do
  context = x[0..t] |> Nx.to_list() |> Enum.join(", ")
  target = y[t] |> Nx.to_number()
  IO.inspect("when input is [#{context}] the target is: #{target}")
end

:ok
```

### Chunks Generation

```elixir
seed = 1337
# how many independent sequences will we process in parallel?
batch_size = 4
# what is the maximum context length for predictions?
block_size = 8

get_batch = fn split ->
  key = Nx.Random.key(seed)

  data = if split == :train, do: train_data, else: val_data

  # Generate random lists (batch size) of indices, each list will
  # be used to "sample" the dataset and plug `x` and `y` tensor starting from
  # each index in the list, the slice has length `block_size` (therefore it needs to be
  # taken into account: `Nx.size(data) - block_size`).
  #
  # Example
  # ix = [5, 10, 40, 60]
  # x = [[46, 47, 51, 57, 43, 50, 44, 1], […], […], […]]
  # y = [[47, 51, 57, 43, 50, 44, 1, 40], […], […], […]]
  #
  # where
  # - x[[0, 0]] = 46 = data[5] (5 is the 1st index in the 1st list of indices)
  # - x[[0, 1]] = 47 = data[5 + 1]
  # - … and so on
  {ix, _new_key} =
    Nx.Random.randint(key, 0, Nx.size(data) - block_size, shape: {batch_size}, type: :u32)

  ix = Nx.to_list(ix)

  # From each index in `ix`, slice the data and extract
  # a 1d tensor of size `block_size`, then stack them all together
  x = Enum.map(ix, fn i -> Nx.slice(data, [i], [block_size]) end) |> Nx.stack()
  y = Enum.map(ix, fn i -> Nx.slice(data, [i + 1], [block_size]) end) |> Nx.stack()

  {x, y}
end

{xb, yb} = get_batch.(:train)

IO.inspect("INPUTS")
IO.inspect(Nx.shape(xb))
IO.inspect(xb)
IO.inspect("TARGETS")
IO.inspect(Nx.shape(yb))
IO.inspect(yb)

IO.inspect("--------------")

# batch dimension (B)
for b <- 0..(batch_size - 1) do
  # time dimension (T)
  for t <- 0..(block_size - 1) do
    context = xb[[b, 0..t]] |> Nx.to_list() |> Enum.join(", ")
    target = yb[[b, t]] |> Nx.to_number()
    IO.inspect("when input is [#{context}] the target is: #{target}")
  end
end

:ok
```

```elixir
# Our input to the transformer

IO.inspect(xb)

:ok
```

## Bigram Language Model

A bigram language model is a type of statistical language model that predicts the probability of a word in a sequence based on the previous word. (source https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/)

```elixir
# Hyperparameters
# B - batch dimension
batch_size = 4
# T - time dimension
block_size = 8

# inputs (xb) and targets (yb) are both {B, T} tensors of integers
shape =
  xb
  |> Nx.to_template()
  |> Nx.shape()

# An embedding layer initializes a kernel of shape `{vocab_size, embedding_size}`
# which acts as a lookup table for sequences of discrete tokens (e.g. sentences).
# Embeddings are typically used to obtain a dense representation of a sparse input space.
# https://hexdocs.pm/axon/0.5.1/Axon.html#embedding/4

bigram_model =
  Axon.input("sequence", shape: shape)
  # vocab_size = embedding_size = 65 
  |> Axon.embedding(65, 65)

Axon.Display.as_graph(bigram_model, Nx.template({batch_size, block_size}, :f32),
  direction: :top_down
)
```

```elixir
{init_fn, predict_fn} = Axon.build(bigram_model, mode: :train)
params = init_fn.(Nx.to_template(xb), %{})

## PREDICTION

# We can make prediction of what's coming next.
#
# `preds` are our `logits` which are basically the scores
# for the next char in the sequence.
# We are predicting what's coming next base on an individual
# identity of a single token and at the moment is possible because
# the token are not aware of the context (they don't talk to each other)
%{prediction: preds} = predict_fn.(params, xb)
IO.inspect(preds, label: "predictions")

## LOSS

# `preds` are of this shape `{b, t, c}`, where `c` stands for `channel`.
# But Axon expect them to be of this shape `{b * t, c}` when computing
# the loss.
{b, t, c} = Nx.shape(preds)
y_pred = Nx.reshape(preds, {b * t, c})

# Same for the target, at the moment have the shape `{b, t}`
# and we want it of shape `{b * t, 1}`
y_true = Nx.reshape(yb, {:auto, 1})

# Let's inspect the shape
y_pred |> IO.inspect(label: "y_pred reshaped")
y_true |> IO.inspect(label: "y_true reshaped")

# Now we can compute the loss
#
# To measure the loss we can use the log loss which
# is implemented in Axon under the name `categorical_cross_entropy`
#
# The `loss` is the cross entropy between the targets `y_true` and
# the predictions `y_pred`.
#
# - we need to compute the loss across the entire minibatch
#   therefore we set the option `reduction: :mean`
# - we need to set `from_logits: true` because `y_pred` is not normalized
# - we need to set `sparse: true` because the inputs are integer values
#   corresponding to the target class
#
# https://hexdocs.pm/axon/0.5.1/Axon.Losses.html#categorical_cross_entropy/3
loss =
  Axon.Losses.categorical_cross_entropy(y_true, y_pred,
    from_logits: true,
    sparse: true,
    reduction: :mean
  )

# We expect the loss to be `-ln(1/65)` where 65 is our vocabulary size,
# and we are getting really close…

IO.inspect(Nx.to_number(loss), label: "loss")
IO.inspect(-:math.log(1 / 65), label: "expected loss")

:ok
```

## Generating text with the bigram model

The `generate` function takes the same inputs which represent the current context of some chars in a batch. The job of the `generate` is to take a `{B, T}` and extend it to be `{B, T + 1}`, `{B, T + 2}` and so on. Basically to continue the generation in all the batch dimension in the time dimension.

```elixir
# Let's assemble the code above inside a module
# for convenience and add the `generate` function

defmodule BigramLanguageModel do
  import Nx.Defn

  def init(vocab_size) do
    # vocab_size = embedding_size
    Axon.input("sequence")
    |> Axon.embedding(vocab_size, vocab_size)
  end

  def forward(model, idx, targets \\ :none) do
    {init_fn, predict_fn} = Axon.build(model, mode: :train)

    params = init_fn.(Nx.to_template(idx), %{})
    %{prediction: preds} = predict_fn.(params, idx)

    if targets == :none do
      {preds, nil}
    else
      # Compute it twice for the sake of keep it simple
      {b, t, c} = Nx.shape(preds)
      y_pred = Nx.reshape(preds, {b * t, c})

      loss = loss(targets, preds)

      {y_pred, loss}
    end
  end

  defn loss(targets, preds) do
    {b, t, c} = Nx.shape(preds)

    # `y_pred` are the `logits` in the video
    y_pred = Nx.reshape(preds, {b * t, c})
    y_true = Nx.reshape(targets, {:auto, 1})

    Axon.Losses.categorical_cross_entropy(y_true, y_pred,
      from_logits: true,
      sparse: true,
      reduction: :mean
    )
  end

  def generate(model, idx, max_new_tokens) do
    for _n <- 0..(max_new_tokens - 1), reduce: nil do
      acc ->
        # get the prediction
        {y_pred, _loss} = BigramLanguageModel.forward(model, idx)

        # focus only on the last time step
        # becomes {B, C}
        logits = y_pred[[.., -1, ..]]

        # apply softmax to get probabilities
        # {B, C}
        probs = Axon.Activations.softmax(logits)

        # sample from the distribution
        # Original in python:
        # `idx_next = torch.multinomial(probs, num_samples=1)`
        #
        # but `multinomial` it is not available in Nx, therfore let's use `argmax` to
        # return the indices of the maximum values.
        # (B, 1)
        idx_next = Nx.argmax(probs, axis: 1, keep_axis: true)

        # {B, T + 1}
        if acc != nil do
          Nx.concatenate([acc, idx_next], axis: 1)
        else
          Nx.concatenate([idx_next], axis: 1)
        end
    end
  end
end
```

📌 Note that `Nx.argmax/2` is used in the last step of the `BigramLanguageModel.generate/3` function because I couldn't find the equivalent of `torch.multinomial` in `Nx`.
I'm might be wrong, but based on my undestanding, the difference is that `torch.multinomial(probs, num_samples=1)` will return the first indices according to when each was sampled, while `Nx.argmax(probs, …)` returns the indices of the maximum value.

References:

* https://pytorch.org/docs/stable/generated/torch.multinomial.html
* https://hexdocs.pm/nx/Nx.html#argmax/2

```elixir
# Text Generation

model = BigramLanguageModel.init(vocab_size)
{logits, loss} = BigramLanguageModel.forward(model, xb, yb)
IO.inspect(Nx.shape(logits), label: "logits shape")
IO.inspect(loss, label: "loss")

# Create the `idx` tensor and
# kick-off the generation starting from `0`.
# `0` encodes the `\n` character which means
# that we are starting the sequence with a 
# new line. 
#
# equivalent to `torch.zeros(1, 1)`
idx = Nx.broadcast(Nx.tensor(0), {1, 1})

max_new_tokens = 100

BigramLanguageModel.generate(model, idx, max_new_tokens)
# Convert our Nx.tensor to Elixir list
|> Nx.to_list()
# Decode the results
|> Enum.map(fn encoded_list -> Minidecoder.decode(encoded_list) end)
# We have only 1 batch, so just unplug the 1st item
|> List.first()
```

👆 At the moment the returned sequence is useless 🗑️

This because:

* the model is "random" and not trained
* the `generate` function is written to be general: we are accumulating the context and use it to feed the model, but then when it is time to predict the next char, we only look at the last char of the context. That's because it is a simple Bigram model.

## Training the Bigram Model (with Axon)

Let's train the model to make it less random! But before starting we need to change the `get_batch` utility function to accept the PRNG (pseudo-random number generator) key as argument in order to get different batches all the time. This is because in `Nx.Random` is stateless (https://hexdocs.pm/nx/Nx.Random.html).

```elixir
get_batch = fn split, batch_size, block_size, pnrg_key ->
  data = if split == :train, do: train_data, else: val_data

  {ix, new_pnrg_key} =
    Nx.Random.randint(pnrg_key, 0, Nx.size(data) - block_size, shape: {batch_size}, type: :u32)

  ix = Nx.to_list(ix)

  x = Enum.map(ix, fn i -> Nx.slice(data, [i], [block_size]) end) |> Nx.stack()
  y = Enum.map(ix, fn i -> Nx.slice(data, [i + 1], [block_size]) end) |> Nx.stack()

  %{batch: {x, y}, pnrg_key: new_pnrg_key}
end
```

```elixir
batch_size = 32
block_size = 8

iterations = 10
seed = 1337
initial_pnrg_key = Nx.Random.key(seed)

# GENERATING THE BATCHES IN THIS WAY IS VERY SLOW!
# I kept the number of iterations low for that reasons
# (to generate 500 batches it took 200 seconds)
%{batches: train_batches} =
  Enum.reduce(0..(iterations - 1), %{pnrg_key: initial_pnrg_key, batches: []}, fn _n, acc ->
    %{pnrg_key: pnrg_key, batches: batches} = acc

    %{batch: {xb, yb}, pnrg_key: new_pnrg_key} =
      get_batch.(:train, batch_size, block_size, pnrg_key)

    %{pnrg_key: new_pnrg_key, batches: batches ++ [{xb, yb}]}
  end)

model = BigramLanguageModel.init(vocab_size)
lr = 1.0e-3
optimizer = Axon.Optimizers.adamw(lr)

model
|> Axon.Loop.trainer(&BigramLanguageModel.loss/2, optimizer)
|> Axon.Loop.run(train_batches, %{}, epochs: 1, iterations: iterations, compiler: EXLA)
```

👆 The batch generation is incredibly slow, it took 200 seconds to generate 500 batches. Let's try to speed it up by making it lazy 💤 and infinite.

_(Kudos to @wdetw for the idea https://github.com/wtedw/gpt-from-scratch/blob/main/gpt-dev.livemd 👏)_

```elixir
seed = 1337

get_streamed_batch = fn split, batch_size, block_size ->
  Stream.resource(
    fn -> Nx.Random.key(seed) end,
    fn pnrg_key ->
      data = if split == :train, do: train_data, else: val_data

      {ix, new_pnrg_key} =
        Nx.Random.randint(
          pnrg_key,
          0,
          Nx.size(data) - block_size,
          shape: {batch_size},
          type: :u32
        )

      ix = Nx.to_list(ix)
      xb = Enum.map(ix, fn i -> Nx.slice(data, [i], [block_size]) end) |> Nx.stack()
      yb = Enum.map(ix, fn i -> Nx.slice(data, [i + 1], [block_size]) end) |> Nx.stack()

      {[{xb, yb}], new_pnrg_key}
    end,
    fn _ -> :ok end
  )
end
```

```elixir
batch_size = 32
block_size = 8
# 10_000
iterations = 100

streamed_batches = get_streamed_batch.(:train, batch_size, block_size)

# Let's inspect 2 batches
IO.inspect(Enum.take(streamed_batches, 2), label: "First 2 batches [{xb, yb}, …]")

model = BigramLanguageModel.init(vocab_size)
lr = 1.0e-3
optimizer = Axon.Optimizers.adamw(lr)

model
|> Axon.Loop.trainer(&BigramLanguageModel.loss/2, optimizer)
|> Axon.Loop.run(streamed_batches, %{}, epochs: 1, iterations: iterations, compiler: EXLA)
```
